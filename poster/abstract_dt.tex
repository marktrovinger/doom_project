\documentclass[a4paper]{article}
%\usepackage{simplemargins}

%\usepackage[square]{natbib}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}

\begin{document}
\pagenumbering{gobble}

\Large
 \begin{center}
Training a Decision Transformer for First-Person Shooter Games\\ 

\hspace{10pt}

% Author names and affiliations
\large
Mark Trovinger$^1$, Perusha Moodley$^2$ \\

\hspace{10pt}

\small  
$^1$) Purdue Fort Wayne\\
trovmr01@pfw.edu\\
$^2$) University of Reading\\
perusha.moodley@pgr.reading.ac.uk\\

\end{center}

\hspace{10pt}

\normalsize

Reinforcement learning algorithms have traditionally been implemented with the goal of maximizing a reward signal. By contrast, Decision Transformer (DT), a proposed architecture uses a transformer model to predict the next action in a sequence. The transformer model is trained on datasets consisting of state, action, return trajectories. The original DT paper examined a small number of environments, five from the Atari domain, and three from continuous control, and one that examined credit assignment. While this gives an idea of what the decision transformer can do, the variety of environments in the Atari domain are limited. In this work, we propose an extension of the environments that decision transformer can be trained on by adding support for the VizDoom environment. We also generate several replay datasets, for a variety of scenarios, and compare the performance of the decision transformer to a more traditional algorithm, Proximal Policy Optimization (PPO). 

\end{document}