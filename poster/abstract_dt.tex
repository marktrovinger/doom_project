\documentclass[a4paper]{article}
%\usepackage{simplemargins}

%\usepackage[square]{natbib}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}

\begin{document}
\pagenumbering{gobble}

\Large
 \begin{center}
Simple Single Page Abstract template\\ 

\hspace{10pt}

% Author names and affiliations
\large
Mark Trovinger$^1$, Perusha Moodey$^2$ \\

\hspace{10pt}

\small  
$^1$) Purdue Fort Wayne\\
trovmr01@pfw.edu\\
$^2$) University of Reading\\
perusha.moodley@pgr.reading.ac.uk\\

\end{center}

\hspace{10pt}

\normalsize

The original DT paper examined a small number of environments, five from the Atari domain, and 5 from continuous control. While this gives an idea of what the decision transformer can do, the variety of environments in the Atari domain are limited. In this work, we propose an extension of the environments that decision transformer can be trained on by adding support for the VizDoom environment. We also generate several replay datasets, for a variety of scenarios, and compare the performance of the decision transformer to a more traditional algorithm, Proximal Policy Optimization (PPO). 

\end{document}